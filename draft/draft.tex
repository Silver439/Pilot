%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%	Author Submission Template for Operations Research (OPRE)
%%	INFORMS, <informs@informs.org>
%%	Ver. 1.00, June 2024
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Use dblanonrev for Double Anonymous Review submission
% Use sglanonrev for Single Anonymous Review submission
% For example, submission to INFORMS Mathematics of Operation Research, MOOR will have
% \documentclass[moor,dblanonrev]{informs4}
%
% \documentclass[opre,dblanonrev]{informs4}
\documentclass[opre,sglanonrev]{informs4}
\usepackage{eqndefns-left} % For checking the display equation width and equation environment definitions %
\RequirePackage{tgtermes}
\RequirePackage{newtxtext}
\RequirePackage{newtxmath}
\RequirePackage{bm}
\RequirePackage{endnotes}

%\OneAndAHalfSpacedXI
\OneAndAHalfSpacedXII % Current default line spacing
%%\DoubleSpacedXI
%%\DoubleSpacedXII

% Optional LaTeX Packages
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
% Private macros here (check that there is no clash with the style)

% Natbib setup for author-number style
\usepackage{natbib}
 \bibpunct[, ]{(}{)}{,}{a}{}{,}%
 \def\bibfont{\small}%
 \def\bibsep{\smallskipamount}%
 \def\bibhang{24pt}%
 \def\newblock{\ }%
 \def\BIBand{and}%


%% Setup of the equation numbering system. Outcomment only one.
%% Preferred default is the first option.
\EquationsNumberedThrough    % Default: (1), (2), ...
%\EquationsNumberedBySection % (1.1), (1.2), ...

%% Setup of theorem styles. Outcomment only one.
%% Preferred default is the first option.
\TheoremsNumberedThrough     % Preferred (Theorem 1, Lemma 1, Theorem 2)
%\TheoremsNumberedByChapter  % (Theorem 1.1, Lema 1.1, Theorem 1.2)
\ECRepeatTheorems  %  

% For new submissions, leave this number blank.
% For revisions, input the manuscript number assigned by the on-line
% system along with a suffix ".Rx" where x is the revision number.
\MANUSCRIPTNO{MOOR-0001-2024.00}

%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%

% Outcomment only when entries are known. Otherwise leave as is and
%   default values will be used.
%\setcounter{page}{1}
%\VOLUME{00}%
%\NO{0}%
%\MONTH{Xxxxx}% (month or a similar seasonal id)
%\YEAR{0000}% e.g., 2005
%\FIRSTPAGE{000}%
%\LASTPAGE{000}%
%\SHORTYEAR{00}% shortened year (two-digit)
%\ISSUE{0000} %
%\LONGFIRSTPAGE{0001} %
%\DOI{10.1287/xxxx.0000.0000}%

% Author's names for the running heads
% Sample depending on the number of authors;
% \RUNAUTHOR{Jones}
% \RUNAUTHOR{Jones and Wilson}
% \RUNAUTHOR{Jones, Miller, and Wilson}
% \RUNAUTHOR{Jones et al.} % for four or more authors
% Enter authors following the given pattern:
%\RUNAUTHOR{}
%\RUNAUTHOR{Smith and Johnson}

% Title or shortened title suitable for running heads. Sample:
% \RUNTITLE{Predictive Maintenance in Manufacturing}
% Enter the (shortened) title:
\RUNTITLE{Module design for pilots training with full flight simulator}

% Full title. Sample:
% \TITLE{Optimal Resource Allocation in Humanitarian Logistics: A Stochastic Programming Approach}
% Enter the full title:
\TITLE{Module Design for Pilots Training with Full Flight Simulator}

% Block of authors and their affiliations starts here:
% NOTE: Authors with same affiliation, if the order of authors allows,
%   should be entered in ONE field, separated by a comma.
%   \EMAIL field can be repeated if more than one author
%\ARTICLEAUTHORS{%
%\AUTHOR{John Doe,\textsuperscript{a} Jane Smith,\textsuperscript{b}}
%\AFF{\textsuperscript{a}Department of Industrial Engineering, University of XYZ, \EMAIL{john.doe@xyz.edu; \textsuperscript{b}Department of Computer Science, University of ABC, \EMAIL{jane.smith@abc.edu}} 
%\AUTHOR{John Smith}
%\AFF{Department of Logistics,
%University of XYZ, \EMAIL{john.smith@xyz.edu}}
%
%\AUTHOR{Emily Johnson}
%\AFF{Department of Logistics,
%University of ABC, \EMAIL{emily.johnson@abc.edu}}
%% Enter all authors
%} % end of the block


\ABSTRACT{
	This study addresses a practical problem in the design of training modules for pilots with Full Flight Simulators (FFS), using operations research techniques. FFSs are highly precise tools that accurately replicate real-world flight scenarios, making them indispensable for pilot training and evaluation. Given their high cost and limited availability, airlines must strategically design modules for pilots’ periodic recurrent assessments. To enhance aviation safety, we formulate a combinatorial optimization objective function to select a set of modules that minimizes a pilot’s lowest score, enabling early detection of potential weaknesses. Our research underscores the importance of accounting for correlations in pilots’ performance across modules. To this end, we develop a suite of optimization methods and provide corresponding managerial insights. Extensive experimental evaluations demonstrate that our approach significantly outperforms traditional methods reliant on intuition and experience.
}% 
\FUNDING{This research was supported by [grant number, funding agency].}

%Supplemental Material:
%Data Ethics & Reproducibility Note:

% Sample
\KEYWORDS{module design, Full Flight Simulators, simulation optimization}

% Fill in data. If unknown, outcomment the field
%\KEYWORDS{Stochastic programming, Decision support,Uncertainty, Disaster response, Optimization} 
%\HISTORY{Received: Month DD, YYYY; Accepted: Month DD, YYYY; Published Online: Month DD, YYYY}

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Text of your paper here

\section{Background}
% Introducing the Full Flight Simulator
The Full Flight Simulator (FFS) is a highly sophisticated flight training device designed to replicate the cockpit environment and operational dynamics of a specific aircraft model with exceptional fidelity \citep{ICAO2015}. 
%Typically mounted on a six-degree-of-freedom hydraulic motion platform, the FFS 
It is capable of simulating the entire spectrum of flight operations, from takeoff and cruise to landing, encompassing both routine procedures and complex scenarios such as engine failures, hydraulic system malfunctions, severe weather conditions, and even rare events like bird strikes or runway contamination \citep{Advancements2024}. The simulation integrates visual, auditory, and tactile feedback, alongside precise instrument readouts and control responses, to provide an immersive experience that closely mirrors actual flight conditions. Certified to the highest standards, such as Level D by aviation authorities like the Federal Aviation Administration (FAA) or the European Union Aviation Safety Agency (EASA), the FFS achieves unparalleled precision, enabling zero flight time training where pilots can qualify without operating an actual aircraft \citep{FAA1995,EASA2020}. However, this extraordinary accuracy comes at a significant cost, with a single FFS unit priced in the tens of millions of dollars, rendering its manufacture and maintenance a substantial financial commitment \citep{CAE7000XR}. Most airlines typically own only a limited number of FFS units outright or instead lease them from manufacturers such as CAE or L3Harris and dedicated flight training centers \citep{IBAAero2023}.

% Explaining the role in pilot training
The primary application of the FFS lies in pilot training and qualification maintenance, with a particular emphasis on mandatory recurrent training to renew licenses. Pursuant to regulations set by the International Civil Aviation Organization (ICAO) and national aviation authorities, pilots are required to undergo recurrent training every six to twelve months, typically conducted in an FFS, to validate their proficiency \citep{Flightradar2024}.
% across a range of scenarios, such as single-engine failures during takeoff or instrument-guided landings in low-visibility conditions  
%This process not only reinforces technical proficiency but also hones decision-making skills and psychological resilience under simulated high-risk conditions. For example, pilots may practice handling emergencies such as single-engine failures during takeoff or instrument-guided landings in low-visibility conditions, fostering muscle memory and situational awareness \citep{Unexpected2018}. Beyond recurrent training, the FFS supports initial pilot training, type rating conversions (e.g., transitioning from a Boeing 737 to an Airbus A320), and annual qualification assessments. 
%Failure to meet the standards in these evaluations may result in the suspension of a pilot’s license \citep{FAA2019}.% until additional training is successfully completed, underscoring the critical role of FFS in upholding aviation safety by allowing pilots to practice high-risk maneuvers in a controlled environment \citep{FAA2019}.
% Discussing the cost and leasing model
%Due to the prohibitive costs and specialized maintenance requirements, most airlines typically own only a limited number of FFS units outright or instead lease them from manufacturers such as CAE or L3Harris and dedicated flight training centers \citep{IBAAero2023}. This leasing model provides airlines with flexible access to simulators while mitigating the burden of substantial upfront investments and ongoing upkeep. For instance, major airlines may maintain a limited number of FFS units tailored to their primary aircraft types at proprietary training facilities, whereas smaller or emerging carriers predominantly rely on leasing arrangements. Such agreements ensure access to simulators that are regularly updated to align with the latest aircraft configurations and software advancements, thereby maintaining operational relevance \citep{Ecological2024}.
% Detailing assessment subjects
During qualification assessments, pilots are evaluated through a series of subjects designed to test their ability to manage diverse operational challenges. These subjects are carefully crafted based on real-world flight data and historical incident analyses%, ensuring relevance to actual flight conditions 
\citep{NTSB2012}. Common subjects include ``Engine Failure on Takeoff,'' where pilots must stabilize the aircraft and execute a safe return to the airport following an engine malfunction immediately after liftoff; ``Low Visibility Approach,'' which tests precision navigation and landing in simulated adverse weather; and ``Emergency Descent and Decompression,'' requiring rapid altitude reduction and oxygen management in response to cabin pressure loss \citep{PilotWorkshops2023}. These assessments, which may span several hours, evaluate technical proficiency, decision-making speed, and crew coordination. Successful completion of these subjects results in license renewal, while deficiencies necessitate further training, ensuring consistent competency across the pilot workforce \citep{EASA2020}.

%Due to the substantial cost of FFS and the critical need for regular pilot assessments, airlines must strategically balance budgetary constraints with the imperative for efficient and effective evaluations. 
%One of the primary objectives of these assessments is to promptly identify deficiencies in a pilot’s performance, enabling targeted training and improvement to ensure operational safety and compliance with regulatory standards. 
%However, the current selection of assessment subjects is often randomized or based on the experiential judgment of airlines, which may overlook specific weaknesses or redundantly test areas where pilots are already proficient, leading to inefficiencies in the evaluation process. 
%This approach risks overlooking specific weaknesses or redundantly testing areas where pilots are already proficient, thereby underutilizing the costly FFS resources. Consequently, there is a pressing need to develop advanced algorithms to optimize the design and implementation of assessment protocols, ensuring that the high-cost simulation environment is used with maximum efficiency and precision.
%To address these challenges, data-driven algorithms can be developed to intelligently select and prioritize assessment subjects based on individual pilot profiles, historical performance data, and operational requirements. Such algorithms could leverage machine learning techniques to analyze patterns in pilot performance across various subjects, identifying correlations between specific skills and scenario-based outcomes. For example, by examining past assessment results, an algorithm might determine that a pilot who struggles with “Low Visibility Approach” is likely to face difficulties in “Instrument Flying” due to shared skill dependencies, such as precise instrument interpretation and spatial awareness. This insight would allow the algorithm to tailor the assessment to focus on these high-risk areas, rather than employing a one-size-fits-all approach. Additionally, algorithms could incorporate real-time data from flight operations, such as recurrent issues observed in actual flights or emerging trends in incident reports, to dynamically adjust the assessment subjects to align with current safety priorities.
%The implementation of such algorithms would not only enhance the efficiency of FFS-based assessments but also maximize the return on investment for airlines leasing or maintaining these expensive systems. By focusing assessments on areas of greatest need, airlines can reduce the time spent in simulators, thereby lowering operational costs without compromising safety. Furthermore, these algorithms could support adaptive training programs by recommending personalized follow-up training modules for pilots who exhibit specific weaknesses. For instance, a pilot struggling with “Emergency Descent and Decompression” could be assigned targeted simulator sessions to practice rapid decision-making under pressure, supplemented by classroom-based refresher courses on relevant procedures. This data-driven approach ensures that training resources are allocated effectively, addressing individual deficiencies while maintaining a high standard of pilot competency.
%Moreover, the development of such algorithms could facilitate predictive analytics to anticipate potential performance issues before they manifest in critical situations. By integrating factors such as a pilot’s experience level, recent flight hours, and performance trends, algorithms could forecast which subjects are most likely to challenge specific pilots, allowing preemptive training interventions. For example, a less experienced pilot might be flagged for additional practice in “Crosswind Landing” based on predictive models indicating a higher likelihood of difficulty due to limited exposure to adverse weather conditions. This proactive approach not only enhances safety but also optimizes the use of FFS resources by reducing the need for repetitive assessments. Ultimately, the adoption of algorithmic solutions in FFS-based pilot assessments represents a transformative step toward precision training, enabling airlines to maintain rigorous safety standards while managing the economic realities of operating high-cost simulation systems.
%This work focuses on selecting the assessment subjects with dedicated optimization approaches. 

Given the high cost of FFS and the critical need for regular pilot assessments, airlines must carefully balance budgetary constraints with the demand for efficient and effective evaluations. However, the current practice of selecting assessment subjects is often random or reliant on airlines' experiential judgment, which can fail to identify specific deficiencies or redundantly evaluate areas of established proficiency. This work aims to develop dedicated optimization approaches to enhance the selection of assessment subjects, maximizing the effectiveness of the evaluation process.

Suppose there is a pool of $M$ different training modules from which $m$ are selected in one training session due to cost restriction. Ideally, these $m$ subjects should collectively cover a diverse range of skills to efficiently identify a pilot’s weaknesses. 
%The goal is to maximize the diagnostic value of the assessment by selecting a diverse set of subjects that collectively cover a broad range of skills while minimizing redundancy. 
Unlike conventional approaches, our strategy incorporates the correlations and variability in pilot performance across these modules, driven by individual differences in skills such as hand-eye coordination, spatial awareness, and stress resilience. For instance, pilots with strong coordination skills often excel in subjects like ``Precision Approach'' and ``Instrument Flying,'' as both require similar competencies, including accurate interpretation of flight instruments, stable aircraft control, and anticipation of spatial positioning \citep{Damos2003}. As a result, high performance in one module frequently predicts success in the other due to shared skill requirements. Conversely, pilots with superior stress management capabilities may outperform in complex emergency scenarios such as ``Multi-Engine Failure'', but struggle in subjects like ``Crosswind Landing'' if their coordination is less developed \citep{Szczepanska2025}. These correlations arise from overlapping skill sets and selecting multiple highly correlated subjects may lead to redundant testing.%, as pilots who excel or struggle in one are likely to perform similarly in the others. 
 Accounting for these correlations is therefore critical in optimizing the selection of assessment modules to enhance training efficiency and safety \citep{Duruaku2024}. 

We illustrate this with a simple example, where we need to select two subjects out of three for a pilot's assessment. From an aviation safety perspective, we aim to select the two subjects that best identify the subject with the lowest score, and thus the assessment metric is the minimum score of the two selected subjects. From an optimization perspective, the following objective function is considered   :
$$
	\min_{x_1,x_2} \mathbb{E}[\min\{Y(x_1),...,Y(x_2) \} ], 
	\label{example}
$$
where $x_1,x_2$ are two selected subjects from the subjects pool $\{A,B,C\}$ and $Y(x_i)$ is the score on $x_i$. The scores of each pilot on these three subjects follow a discrete distribution with three scenarios (shown in Table \ref{tab:example}), each with probability 1/3.
\begin{table}[ht]
	\centering
	\caption{Discrete distribution of the scores}
	\label{tab:example}
	\begin{tabular}{cccc} 
		Scenario & A score & B score & C score \\ \hline 
		1 & 9 & 9.5 & 7.5\\
		2 & 6 & 5 & 8\\
		3 & 10 & 9.5 & 10\\
		Expectation & 8.33 & 8 & 8.5\\
	\end{tabular}
\end{table}
From the table, we can observe that the correlation between scores for subjects A and B is significantly higher than that between A and C or B and C, due to the differing skill requirements for the subjects, as previously discussed. If correlations are ignored, the first two subjects might be chosen (due to their lower mean scores). However, calculations reveal that the latter two subjects should be selected. This is precisely because their lower correlation allows them to more independently yield lower scores, increasing the likelihood of identifying the pilot’s weakest performance. Therefore, we need to design an optimization method that fully accounts for the correlations between scores of different subjects.

Addressing this problem presents multiple challenges. First, computing the expected minimum value of several correlated distributions is inherently complex, particularly when selecting an optimal subset of $m$ modules from a larger set to minimize this value. Second, in practical applications, the expected values of these distributions are likely unknown. While we can infer correlations between scores based on the skill requirements of different subjects, estimating the mean scores of pilots across these subjects is considerably more difficult. Hence, an approach that simultaneously learns from data of previous test results and optimizes the selection process is necessary.

To address the first challenge, we assume that the scores follow a multivariate normal distribution (a reasonable model for characterizing examination scores \citep{Ross2014} and has been validated through real-world FFS data). By analyzing the impact of parameters—such as means, variances, and correlations—on the target function, we derive insights to guide module selection. Furthermore, we identify the target function as a submodular problem, enabling the use of a greedy algorithm for approximate computation. To improve computational efficiency, we propose a heuristic algorithm informed by these analytical results. To tackle the second challenge, we develop a combinatorial multi-armed bandit (MAB) algorithm based on a Bayesian framework. This algorithm assigns a prior distribution to the score means, which is continuously updated through iterative testing. In each iteration, an upper confidence bound approach is used to select the module combination to be tested, and we prove the algorithm’s regret bound to ensure its theoretical robustness.

This paper addresses a problem of significant practical relevance, representing an innovative effort to apply optimization and operations research algorithms to real-world challenges. Previously, the design of pilot training modules has relied heavily on intuition and experience. Beyond its practical significance, the optimization problem explored here also carries substantial theoretical value, presenting multiple challenges as outlined earlier. Moreover, while related to existing research domains, it exhibits notable distinctions. Our work primarily aligns with two categories of optimization studies. The first is simulation-based optimization \citep{Fu2015}, which employs simulation models to support decision-making in real-world systems (FFS is a highly precise simulation tool). Certain simulation optimization studies account for correlations in experimental results when selecting multiple design alternatives \citep{xie2016bayesian}. These correlations primarily arise from the use of common random numbers in computer simulations. However, no prior work in this field has targeted the same objective function as ours, as most focus on evaluating a single design’s performance without an internal minimization operation. The second category is combinatorial multi-armed bandit research \citep{Chen2013}, where some studies select multiple alternatives and use the best as the evaluation metric, akin to our objective function \eqref{example}. Yet, to the best of our knowledge, none have addressed correlations among alternatives. These correlations significantly complicate the problem while offering unique managerial insights.

The remainder of the paper is organized as follows. Section \ref{related works} reviews the related works. Section \ref{problem formulation} formulates the objective function. Section \ref{know means} offers some insights and heuristic algorithms for scenarios where expectations are known. Section \ref{unknown means} presents an algorithm that learns expectations during the optimization process, supported by theoretical guarantees. Section 6 reports numerical experiments. Finally, Section 7 concludes the paper.

%Specifically, subjects with high correlation—such as “Precision Approach” and “Instrument Flying,” which both rely heavily on spatial awareness and instrument interpretation—tend to yield similar performance outcomes. Selecting multiple highly correlated subjects may result in redundant testing, as pilots who excel or struggle in one are likely to perform similarly in the others, thus reducing the likelihood of uncovering unique deficiencies. Therefore, our approach prioritizes selecting subjects with lower inter-correlation to ensure a comprehensive evaluation of a pilot’s skill set within the limited testing framework.

% Addressing individual differences
%Individual differences, such as hand-eye coordination, spatial awareness, and stress resilience, contribute to variability in pilot performance across these subjects. For instance, pilots with strong coordination skills often excel in subjects like ``Precision Approach'' and ``Instrument Flying,'' as both require similar competencies, including accurate interpretation of flight instruments, stable aircraft control, and anticipation of spatial positioning \citep{Damos2003}. Consequently, high performance in one of these subjects is often correlated with success in the other due to overlapping skill demands. Conversely, pilots with superior stress management capabilities may outperform in complex emergency scenarios such as ``Multi-Engine Failure,'' yet struggle in subjects like ``Crosswind Landing'' if their coordination is less developed \citep{Szczepanska2025}. These performance correlations stem from shared skill sets, such as spatial orientation, which influence outcomes across related tasks. Recognizing these individual variations enables training programs to be tailored to enhance each pilot’s strengths and address weaknesses, thereby optimizing overall competency and safety \citep{Duruaku2024}.

%Full flight simulator (FFS) is a sophisticated simulation system to simulate all aircraft systems that are accessible from the flight deck and are critical to training. For instance, it can simulate the force feedback for the pilot's flight controls, the avionics system, the communication system, the cockpit sounds, the aerodynamics, and the ground handling. It prepares pilots for realistic flight situations and is used for pilots training.

%FFS has extremely high fidelity and is typically expensive the run. This will limit the number of sessions each pilot can try with FFS. Apart from initial training, the pilots must carry out recurrent training at regular intervals (such as every six months) in order to retain their qualification. In addition, pilots’ reactions to different situations and flight skills are different. Therefore, it is important to design efficient personal training sessions for pilots to detect and improve their inadequacies.
\section{Related works}
\label{related works}
We review relevant works in three aspects: existing approaches for pilot training modules design, simulation-based optimization, and combinatorial MAB.

\section{Problem Formulation}
\label{problem formulation}
Our goal is to efficiently identify pilots’ deficiencies through rational module design, with these deficiencies measured by their scores during the module evaluation process. Suppose there are $M$ different training modules from which $m$ are selected in one training session due to the time and cost restriction. Then, we select $m$ modules to maximally expose a pilot’s deficiencies, such that the minimum score across these $m$ modules is as low as possible. The optimization problem can be formulated as follows:
\begin{equation}
	\min_{x_1,...,x_m} \mathbb{E}_\xi[\min\{Y(x_1,\xi),...,Y(x_m,\xi) \} ], 
	\label{obj}
\end{equation}
where $x_1,...,x_m$ represent the $m$ modules in the training session, taken from the module set $\mathcal{X} = \{x_1,...,x_M\}$, and $Y(x_1,\xi)$ is the score the pilot obtain in module $x_i$. It is evaluated through simulation at FFS. In this model, we assume the score vector in the population $\{Y(x_1),...,Y(x_M)\}$ follows a multivariate normal distribution. For each specific person, his score is a sample from this distribution. In equation \eqref{obj}, the random vector $\xi$ is used to illustrate the randomness in this multivariate normal distribution. The inner minimization is to find the minimum score one pilot obtain across the given $m$ training modules. We take the minimum value as we need to find the weakness of the pilots considering flight safety. The outer minimization is the find the best set of training sessions in detecting the pilot inadequacy.
%This formulation is not for `personal' module design, as we choose the modules that performs the best in expectation over the whole population. 

%This problem involves two stages. The first stage is to learn the multivariate normal distribution as well as to identify the optimal modules. This is the simulation optimization process. In the two stage, we deploy the recommended session to train the pilots. 

%In the first stage, we use the Bayesian approach. Denote $\mathbf{Y}=\{Y_1,...,Y_M\}$ as $\{Y(x_1),...,Y(x_M)\}$ for convenience. Our assumption is that $\mathbf{Y} \sim \mathcal{N}(\rho, \Sigma)$, where $\rho$ is the $m\times 1$ mean vector and $\Sigma$ is the $m\times m$ covariance matrix. We can apply the conjugate prior for $\rho$ and $\Sigma$. In iteration $t$, we select $l$ modules ($l$ can be different to $m$ and $M$) $x^t_1, ..., x^t_l$ and obtain a score vector observation $Y^t=\{Y(x^t_1,\xi^t),...,Y(x^t_l,\xi^t) \}$. $Y^t$ is then used to update the posterior. 

%\section{Critical questions for now}
%\begin{enumerate}
%	\item $Y^t$ is a partial observation vector of $\mathbf{Y}$ if $l\neq M$. How to update the posterior with $Y^t$?
%	\item We need to design a criterion to select points. 
%\end{enumerate}
\section{Insights and Algorithm with Known Score Means}
\label{know means}
We first consider an ideal scenario for solving Problem \eqref{obj}, where all parameters of the score distributions for all modules are fully known. That is, we aim to select a limited number of dimensions from a completely known multivariate normal distribution such that the expected value of the minimum score across these dimensions is minimized. It should be noted that, in general, this objective function cannot be simplified into a more computationally convenient form. Existing studies provide closed-form solutions for the expected minimum of two normal distributions \citep{nadarajah2008exact}. However, for more than two normal distributions, prior research remains limited. \cite{clark1961greatest} relied on numerical approximation, that recursively applies a three-variate formula, to compute the expected value of their minimum and has analyzed the accuracy of these approximations through numerical validation. Such computational methods are time-consuming, particularly in our optimization scenario, where we need to repeatedly select $m$ modules and compute the expected value of their minimum scores. Therefore, we aim to derive managerial insights from simple, analytically tractable examples and leverage these insights to develop efficient heuristic methods. This section is divided into two parts: in Section \ref{insights}, we use two simplified examples to obtain managerial insights; in Section \ref{submodular}, we demonstrate that the objective function is submodular, enabling the use of a greedy algorithm to achieve an efficient and theoretically guaranteed approximation; in Section \ref{heuristic}, we utilize these insights to develop an efficient heuristic method based on greedy approach.

\subsection{Insights from two simple examples}
\label{insights}
This subsection examines two simplified examples to investigate how parameters in a multivariate normal distribution—specifically the mean, correlation, and variance—affect the expected minimum value. The examples include a bivariate normal distribution (Section \ref{bivariate normal}) and a multi-variate scenario with same means, variances, and correlations (Section \ref{multi-variate}). Despite their simplicity, the conclusions and insights from these examples align with intuition and are corroborated by subsequent numerical experiments.

\subsubsection{Bivariate normal example}
\label{bivariate normal}

%We need to compare $\text{E}_{\xi_1, \xi_2}[\min \{\mu_1+A\xi_1, \mu_2+B_1\xi_1+B_2\xi_2 \}]$ and $\text{E}_{\xi_1, \xi_2}[\min \{\mu_1+A\xi_1, \mu_2+B\xi_2 \}]$, where $\xi_1, \xi_2$ are independent standard normal random variable. All the coefficients are positive and that $B_1^2+B_2^2=B^2$.

For any two Gaussian random number $X_1\sim N(\mu_1,\sigma^2_1)$, $X_2\sim N(\mu_2,\sigma^2_2)$ with correlation $\rho$, the expectation $\text{E}[\min\{X_1, X_2\}]$ takes the following form \citep{clark1961greatest}:
$$
\text{E}[\min\{X_1, X_2\}] = \mu_1\Phi(\frac{\mu_2-\mu_1}{\theta}) + \mu_2\Phi(\frac{\mu_1-\mu_2}{\theta}) - \theta \phi(\frac{\mu_2-\mu_1}{\theta} ),
$$
where $\phi$, $\Phi$ are cdf and pdf for standard normal distribution, respectively, and $\theta = \sqrt{\sigma^2_1+\sigma^2_2-2\rho \sigma_1\sigma_2} = \sqrt{\text{var}(X_1-X_2)}$.

%We define $X_1=\mu_1+A\xi_1 \sim N(\mu_1, A^2)$, $X_2=\mu_2+B_1\xi_1+B_2\xi_2 \sim N(\mu_2, B_1^2+B_2^2)$, $X_3=\mu_2+B\xi_2 \sim N(\mu_2,B^2)$. Thus, 
%$$
%\text{cov}(X_1, X_2) = AB_1, \text{cov}(X_1, X_3) = 0.
%$$
%$$
%\text{var}(X_1-X_2) = A^2+B_1^2+B_2^2-2AB_1, \text{var}(X_1- X_3) =A^2+B^2.
%$$
%Therefore,
%$$
%\text{E}[\min\{X_1, X_2\}]= \mu_1\Phi(\frac{\mu_2-\mu_1}{\theta_1}) + \mu_2\Phi(\frac{\mu_1-\mu_2}{\theta_1}) - \theta_1 \phi(\frac{\mu_2-\mu_1}{\theta_1} ),
%$$
%$$
%\text{E}[\min\{X_1, X_3\}]= \mu_1\Phi(\frac{\mu_2-\mu_1}{\theta_2}) + \mu_2\Phi(\frac{\mu_1-\mu_2}{\theta_2}) - \theta_2 \phi(\frac{\mu_2-\mu_1}{\theta_2} ),
%$$
%where $\theta_1=\sqrt{A^2+B_1^2+B_2^2-2AB_1}$, $\theta_2=\sqrt{A^2+B^2}$.
%Denote:
%$$
%f(\theta) =  \mu_1\Phi(\frac{\mu_2-\mu_1}{\theta}) + \mu_2\Phi(\frac{\mu_1-\mu_2}{\theta}) - \theta \phi(\frac{\mu_2-\mu_1}{\theta} ) = (\mu_1-\mu_2)\Phi(\frac{\mu_2-\mu_1}{\theta}) +\mu_2- \theta \phi(\frac{\mu_2-\mu_1}{\theta} ).
%$$
%We have
%$$
%f'(\theta) = (\mu_1-\mu_2)\phi(\frac{\mu_2-\mu_1}{\theta})\frac{\mu_2-\mu_1}{-\theta^2}-\phi(\frac{\mu_2-\mu_1}{\theta})-\theta\phi(\frac{\mu_2-\mu_1}{\theta})\frac{\mu_2-\mu_1}{-\theta} \frac{\mu_2-\mu_1}{-\theta^2}
%$$
%$$
%=-\phi(\frac{\mu_2-\mu_1}{\theta}) <0
%$$
%
%Therefore, $f_\theta$ is a decreasing function w.r.t. $\theta$. We have the following conclusions:
%\begin{enumerate}
%	\item When $B_1^2+B_2^2=B^2$ and all coefficients are positive, we have $\theta_1=\sqrt{A^2+B^2-2AB_1}<\theta_2$. Hence, $\text{E}[\min\{X_1, X_2\}]>\text{E}[\min\{X_1, X_3\}]$.
%	\item When $B_1^2+B_2^2=B^2$ and $B_1<0$, we have $\theta_1=\sqrt{A^2+B^2-2AB_1}>\theta_2$. Hence, $\text{E}[\min\{X_1, X_2\}]<\text{E}[\min\{X_1, X_3\}]$.
%\end{enumerate}
%
%Another way of presenting these results. Suppose $X_1\sim N(\mu_1,\sigma^2_1)$, $X_2\sim N(\mu_2,\sigma^2_2)$ with correlation $\rho$ and $\theta = \sqrt{\sigma^2_1+\sigma^2_2-2\rho \sigma_1\sigma_2} = \sqrt{\text{var}(X_1-X_2)}$. 

Denote $f=\text{E}[\min\{X_1, X_2\}] $. We have:
\begin{equation}
	\frac{\partial f}{\partial \theta} = -\phi(\frac{\mu_2-\mu_1}{\theta})<0,
\end{equation}
\begin{equation}
	\label{rho}
	\frac{\partial f}{\partial \rho} = \frac{\partial f}{\partial \theta} \frac{\partial \theta}{\partial \rho} =  \frac{\sigma_1\sigma_2}{\theta}\phi(\frac{\mu_2-\mu_1}{\theta})>0,
\end{equation}
\begin{equation}
	\label{sigma}
	\frac{\partial f}{\partial \sigma_1} = \frac{\partial f}{\partial \theta} \frac{\partial \theta}{\partial \sigma_1} = -\frac{\sigma_1-\rho\sigma_2}{\theta} \phi(\frac{\mu_2-\mu_1}{\theta}),
\end{equation}
\begin{equation}
	\label{mu}
	\frac{\partial f}{\partial \mu_1} = \Phi( \frac{\mu_2-\mu_1}{\theta})>0.
\end{equation}

We summarize the conclusions is the following lemma:
\begin{lemma}
	In the bivariate normal example, the expectation of the minimum value is smaller when the two variables have smaller expectations and correlations.
\end{lemma}

The relation between $f$ and the variance for each individual variable is not monotone. We can, however, derive some special cases:
	\begin{enumerate}
		\item When $\rho<\frac{\sigma_1}{\sigma_2}$ (specifically when $\rho<0$), a larger value of $\sigma_1$ is preferred.
		\item When $\rho>\frac{\sigma_1}{\sigma_2}$, smaller value of $\sigma_1$ is preferred. This result may appear counter-intuitive. To gain insight, consider a special case where $\rho = 1$ and $\sigma_1<\sigma_2$. In this scenario, $X_1$ and $X_2$ are positively linear dependent, with $ X_1 = \frac{\sigma_1}{\sigma_2} X_2$. Consequently, $X_1$ and $X_2$ always share the same sign. When $X_1, X_2 > 0$, the minimum is $\min\{X_1, X_2\} = X_1 > 0$, which increases with $\sigma_1$. When $X_1, X_2 < 0$, the minimum is $\min\{X_1, X_2\} = X_2 < 0$, which is independent of $\sigma_1$. Thus, a larger $\sigma_1$ increases the objective function value in this case. Although a larger $\sigma_1$ reduces $X_1$ when negative, these negative values do not contribute to the objective function as long as $\sigma_1 < \sigma_2$. This effect diminishes slightly as $\rho$ deviates from 1, but negative values of $X_1$ remain less likely to influence the objective function until $\rho < \frac{\sigma_1}{\sigma_2}$.
		
		%We can find that they always have the same sign and when $X_1$ and $X_2>0$, $\min\{X_1,X_2\}=X_1>0$, which increases with $\sigma_1$. When $X_1$ and $X_2<0$,  $\min\{X_1,X_2\}=X_2<0$, which does not change with $\sigma_1$. Therefore, in this case, a larger value of $\sigma_1$ will increase the objective value. Although a larger value of $\sigma_1$ will reduce $X_1$ when it is negative, these negative values, however, are not counted in the objective function as long as $\sigma_1<\sigma_2$. This situation will slightly gets better when $\rho$ deviates from 1, but still makes less negative values of $X_1$ to be counted in the objective function until $\rho<\frac{\sigma_1}{\sigma_2}$.
	\end{enumerate}


\subsubsection{Multi-variate normal example}
\label{multi-variate}

We consider the following scenario: $m$ normal random numbers $X_1,...,X_m$ with equal mean $\mu$ and equal variance $\sigma^2$. The correlation between each pair of variables is $\rho\geq 0$. We would like to explore the relationship between the objective function  $f = \text{E}[\min\{X_1,...,X_m\}]$ and $\rho$, $\mu$, and $\sigma$.

In this case, each random variable can be represented as $X_i = \mu + \sigma(\sqrt{\rho}W + \sqrt{1-\rho} N_i )$, where $W$ and $N_1,...,N_m$ are i.i.d. standard normal random variables. Then,
$$f = \text{E}[\min\{X_1,...,X_m\}] =  \text{E}[\min\{\mu + \sigma(\sqrt{\rho}W + \sqrt{1-\rho} N_1 ),...,\mu + \sigma(\sqrt{\rho}W + \sqrt{1-\rho} N_m )\}] $$
$$=\mu+ \sigma\text{E}[\min\{\sqrt{\rho}W + \sqrt{1-\rho} N_1 ,...,\sqrt{\rho}W + \sqrt{1-\rho} N_m \}]  $$
$$=\mu + \sigma\text{E}[\sqrt{\rho}W+ \sqrt{1-\rho} \min\{N_1,...,N_m\} ] =\mu + \sigma\sqrt{1-\rho} \text{E}[\min\{N_1,...,N_m\} ]. $$
%This had no analytical solution. Some special cases:
%\begin{enumerate}
%	\item When $\rho =1$, $f=\mu$.
%	\item When $\rho =0$, $f = \mu+ \sigma\text{E}[\min\{N_1 ,...,N_m \}]$. 
%\end{enumerate}
Noted that $\text{E}[\min\{N_1 ,...,N_m \}]$ is a constant for a given $m$. It is negative and can be approximated as 
$$-\sqrt{2 \ln m} + \frac{\ln \ln m + \ln 4\pi}{ 2 \sqrt{2 \ln m}}. $$
Hence, $f\approx \mu - \sigma\sqrt{1-\rho}\sqrt{2 \ln m}$. 

We summarize the insights from this example in the following lemma.

\begin{lemma}
	In the simple multi-variate normal example, the expectation of the minimum value is smaller with smaller $\mu$, larger $\sigma$ or smaller $\rho$.
\end{lemma}

For a general multi-variate normal distribution, analyzing the impact of the covariance matrix on the objective function $   f   $ using simple algebra is challenging. However, we can derive the following lemma regarding the influence of the means.
\begin{lemma}
	In a general multi-variate normal distribution, if the covariance matrix is fixed, the value of $f$ reduces if the mean vector gets smaller pointwisely.
\end{lemma}

We summarize the managerial insights derived from these examples for decision-making as follows. To achieve a smaller value for the objective function, we should generally select: 
\begin{enumerate}
	\item $m$ Gaussian variables with smaller means;
	\item variables with lower correlations;
	\item variables with larger variances.
\end{enumerate}
According to Lemma 3, when other factors remain constant, a smaller mean vector directly reduces the expected minimum value. Furthermore, regarding the other two points, the lower the correlations and the larger the variances of the $m$ variables, the more independently each variable can take potentially smaller values, thereby reducing the minimum value.

%Intuitively, when $\rho$ decreases, the $m$ variables are less correlated and thus each of them has larger probability to take smaller values more independently. Therefore, the objective value increases as $\rho$. 

\subsection{Demonstration of sub-modular objective function}
\label{submodular}

\subsection{Heuristic algorithms}
\label{heuristic}

\section{A Combinatorial MAB Algorithm for Unknown Score Means}
\label{unknown means}
We assume that the score has Gaussian noise: $Y(x) = Z(x) + \mathcal{N}(0,\sigma(x))$ and the joint distribution for $Y(x)_{x\in\{\mathcal{X}\}}$ is a multivariate normal distribution with mean vector $\theta = [Z(x_1),...,Z(x_M)]^T$ and covariance matrix $\Lambda$. We use a Bayesian approach to model $\theta$ and assume a normal prior:
$$\theta \sim \mathcal{N}(\mu_0, \Sigma_0). $$
As with \cite{xie2016bayesian}, we assume $\Lambda, \mu_0, \Sigma_0$ are known.

The $i$th entry of a vector $v$ is denoted as $v(i)$ and the $(i,j)$th entry of a matrix $M$ is denoted as $M(i,j)$. For an ordered collection of $m$ alternatives ${\bf x}=(x_1,...,x_m)$ with element $x_i \in \{1,...,M\}$ for each $i$, we use $v(\bf x)$ to denote the subvector of $v$ with the $i$th entry equal to $v(x_i)$. We further denote by $M(\bf x, x')$ the $m$-by-$m$ submatrix of $M$ with the $(i,j)$th entry equal to $M(x_i, x_j')$.  

We consider a situation where in each iteration $n$, one pilot will attend $m$ different modules ${\bf x}_n = (x_{n,1},...,x_{n,m})^T$ and we obtain his score vector ${\bf y}_n = (Y(x_{n,1}),...,Y(x_{n,m}))^T$. The conditional distribution of ${\bf y}_n$ is:
$${\bf y}_n | \theta,{\bf x}_n \sim \mathcal{N}(\theta({\bf x}_n), \Lambda({\bf x}_n,{\bf x}_n)). $$ 
Let $\mathbb{X}_n = ({\bf x}_1^T,...,{\bf x}_n^T )$ denote the concatenation of the design points of the previous $n$ iteration and similarly $\mathbb{Y}_n =({\bf y}_1^T,...,{\bf y}_n^T )^T$. Then, the posterior distribution for $\theta$ is:
$$\theta_n|\mathbb{X}_n,\mathbb{Y}_n  \sim \mathcal{N}(\mu_n, \Sigma_n),$$
where for any vector ${\bf x} = (x_1, x_2,...,x_m)$,
$$
	\mu_n({\bf x}) = \mu_0({\bf x}) + \Sigma_0(\bf x, \mathbb{X}_n)(\Sigma_0(\mathbb{X}_n,\mathbb{X}_n) + \Gamma_n)^{-1}(\mathbb{Y}_n-\mu_0(\mathbb{X}_n)),
$$
$$
	\Sigma_n({\bf x},{\bf x}) = \Sigma_0({\bf x},{\bf x}) - \Sigma_0(\bf x, \mathbb{X}_n)(\Sigma_0(\mathbb{X}_n,\mathbb{X}_n) + \Gamma_n)^{-1}\Sigma_0(\mathbb{X}_n, \bf x),
$$
where $\Gamma_n$ is the block diagonal matrix with $n$ blocks: $\Lambda({\bf x}_1, {\bf x}_1),...,\Lambda({\bf x}_n, {\bf x}_n)$.

We adopt the Expected Improvement (EI) acquisition function to select the $m$ courses for the next iteration $n+1$. For any candidate vector ${\bf x}=(x_1,...,x_m)$,
$$ 
\text{EI}({\bf x}) = \mathbb{E}_{\theta_n}[(g_c-G({\bf x}))^+ | \theta_n],
$$
where $(a)^+=a$ if $a\geq 0$ and $(a)^+=0$ otherwise. When $\theta_n$ takes a value $\tilde{\theta_n}$, we define $G({\bf x})$ as:
$$
G({\bf x}) = \mathbb{E}_\xi[\min\{Y(x_1,\xi),...,Y(x_m,\xi) \} ],
$$
where $\{Y(x_1),...,Y(x_m)\} \sim \mathcal{N}(\tilde{\theta_n}({\bf x}), \Lambda({\bf x},{\bf x}))$ and $\{Y(x_1,\xi),...,Y(x_m,\xi)\}$ is a random sample from this distribution. In the EI function, $g_c$ is the current best value: $g_c = \min \{G({\bf x}_1),...,G({\bf x}_n)\}$.

We next explain how to compute $g_c$. For any vector ${\bf x}_i, 1\leq i \leq n$, we approximate $G({\bf x}_i)$ through sample average approximation (SAA). Specifically, we generate samples of $\{Y(x_{i,1},\xi),...,Y(x_{i,m},\xi)\}$ from distribution $\mathcal{N}(\mu_n({\bf x}_i), \Lambda({\bf x}_i,{\bf x}_i))$. Here, as we already have observations at ${\bf x}_i$, we use the posterior mean $\mu_n({\bf x}_i)$ as if it were the true value of $\theta_n({\bf x}_i)$ to compute the current best value. Similar approach has been widely used for ordinary stochastic GP based optimization algorithms. The samples can then be generated as follows. Suppose $\Lambda({\bf x}_i,{\bf x}_i) = B_iB_i^T$ and ${\bf z}_j\in \mathbb{R}^{m\times 1}$ is a random draw from $m$ iid standard normal distribution. The $j$-th random samples can be represented as: $(Y(x_{i,1},\xi_j),...,Y(x_{i,m},\xi_j))^T = \mu_n({\bf x}_i) + B_i{\bf z}_j$ and $Y(x_{i,k},\xi_j) = \mu_n({x}_{i,k}) + B_i^k{\bf z}_j$, where $B_i^k$ is the $k$th row of $B_i$. Therefore, we have
$$ 
G({\bf x}_i) \approx \frac{1}{J}\sum_{j=1}^{J} \min \mu_n({\bf x}_i) + B_i{\bf z}_j = \frac{1}{J}\sum_{j=1}^{J} \min \{\mu_n({x}_{i,1}) + B_i^1{\bf z}_j, ...,\mu_n({x}_{i,m}) + B_i^m{\bf z}_j  \}.
$$

To compute EI for ${\bf x}$, we should further take care of the outer expectation with respect to the posterior distribution of $\theta$. We can use similar approach as above the generate samples from the posterior distribution of $\theta_n({\bf x})$ and obtain the following SAA form:
$$
\text{EI}({\bf x}) \approx \frac{1}{K}\sum_{k=1}^{K} (g_c-  \frac{1}{J}\sum_{j=1}^{J} \min \theta_n({\bf x}) + A{\bf \tilde{z}}_k + B{\bf z}_j )^+,
$$ 
where $\Sigma_n({\bf x},{\bf x}) = AA^T$, $\Lambda({\bf x},{\bf x}) = BB^T$, and both ${\bf \tilde{z}}_k$ and ${\bf z}_j$ are iid standard normal vectors of length $m$.



%\begin{algorithm}
%\begin{algorithmic}[1]
%\caption{Random Forest Training II}
%\Procedure{DecisionTree}{$X, y$}
%    \If{\textsc{StoppingCondition}$(X, y)$}
%        \State \textbf{return} LeafNode$(y)$
%    \Else
%        \State $(\text{feature}, \text{threshold}) \gets \textsc{FindBestSplit}(X, y)$
%        \State $\textit{left\_indices} \gets X[\text{feature}] \leq \text{threshold}$
%        \State $\textit{right\_indices} \gets X[\text{feature}] > \text{threshold}$
%        \State $\textit{left\_subtree} \gets$ \Call{DecisionTree}{$X[left\_indices], y[left\_indices]$}
%        \State $\textit{right\_subtree} \gets$ \Call{DecisionTree}{$X[right\_indices], y[right\_indices]$}
%        \State \textbf{return} TreeNode$(\text{feature}, \text{threshold}, left\_subtree, right\_subtree)$
%    \EndIf
%\EndProcedure
%\Statex
%
%\Procedure{Predict}{$\text{forest}, x$}
%    \State predictions $\gets []$
%    \For{$\text{tree}$ \textbf{in} $\text{forest}$}
%        \State \textbf{append} \Call{TreePredict}{$\text{tree}, x$} to predictions
%    \EndFor
%    \State \textbf{return} \Call{AggregatePredictions}{predictions}
%\EndProcedure
%\Statex
%
%\Procedure{TreePredict}{$\text{tree}, x$}
%    \If{$\text{tree}$ is a leaf node}
%        \State \textbf{return} $\text{tree.value}$
%    \Else
%        \If{$x[\text{tree.feature}] \leq \text{tree.threshold}$}
%            \State \textbf{return} \Call{TreePredict}{$\text{tree.left\_subtree}, x$}
%        \Else
%            \State \textbf{return} \Call{TreePredict}{$\text{tree.right\_subtree}, x$}
%        \EndIf
%    \EndIf
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}


%\THEEndNotes
%\begingroup \parindent 0pt \parskip 0.0ex \def\enotesize{\normalsize} \theendnotes \endgroup

% Appendix here
% Options are (1) APPENDIX (with or without general title) or
%             (2) APPENDICES (if it has more than one unrelated sections)
% Outcomment the appropriate case if necessary
%
% \begin{APPENDIX}{<Title of the Appendix>}
% \end{APPENDIX}
%
%   or
%
% \begin{APPENDICES}
% \section{<Title of Section A>}
% \section{<Title of Section B>}
% etc
% \end{APPENDICES}

% Acknowledgments here
%\ACKNOWLEDGMENT{We would like to express our sincere gratitude to [acknowledge individuals, organizations, or institutions] for their invaluable contributions to this research. We are also grateful to [mention any additional acknowledgements, such as technical assistance, data providers, or colleagues] for their support and assistance throughout the course of this work.}


% References here (outcomment the appropriate case)

% CASE 1: BiBTeX used to constantly update the references
%   (while the paper is being written).
%\bibliographystyle{informs2014} % outcomment this and next line in Case 1
%\bibliography{<your bib file(s)>} % if more than one, comma separated

\bibliographystyle{informs2014} % outcomment this and next line in Case 1
\bibliography{sample} % if more than one, comma separated

% CASE 2: BiBTeX used to generate mypaper.bbl (to be further fine tuned)
%\input{mypaper.bbl} % outcomment this line in Case 2

%If you don't use BiBTex, you can manually itemize references as shown below.

%\bibliographystyle{nonumber}


%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%



