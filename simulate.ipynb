{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c0c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import multivariate_normal\n",
    "from util.data_generate import *\n",
    "from util.methods import *\n",
    "import csv\n",
    "#worst\n",
    "p = 50\n",
    "betas = ['smallest','cluster','dynamic']\n",
    "seed, J, n_iter, num_repeats = 1, 10000, 150, 30\n",
    "np.random.seed(seed=seed)\n",
    "Sigma1 = np.loadtxt('Data_generate/Sigma1.txt')\n",
    "Sigma1 = Sigma1 * 4\n",
    "\n",
    "Sigma2 = np.loadtxt('Data_generate/group_cov.txt')\n",
    "std2 = np.sqrt(np.diag(Sigma2))\n",
    "R2 = Sigma2 / np.outer(std2, std2)\n",
    "mu = np.loadtxt('Data_generate/mu.txt')\n",
    "\n",
    "for m in [16,]:\n",
    "    U3, ids3 = SAA(p, m, mu, std2, R2, Sigma2, J=J)\n",
    "    z_samples = np.random.randn(J*10, m)\n",
    "    G_real = np.mean(np.min(mu[ids3] + (U3 @ z_samples.T).T, axis=1))\n",
    "    cluster_labels = cluster_subjects(R2, n_clusters=m, method='average')\n",
    "    for beta in betas:\n",
    "\n",
    "        csv_path = f'{p}_{m}_{beta}.csv'\n",
    "        csv_path1 = f'{p}_{m}_{beta}_pre.csv'\n",
    "\n",
    "        # Initialize CSV\n",
    "        with open(csv_path, 'w') as f:\n",
    "            f.write(','.join(['Repetition'] + [f'Step_{i}' for i in range(1, n_iter+1)]) + '\\n')\n",
    "        with open(csv_path1, 'w') as f:\n",
    "            f.write(','.join(['Repetition'] + [f'Step_{i}' for i in range(1, n_iter+1)]) + '\\n')\n",
    "\n",
    "        for repeat in tqdm(range(num_repeats)):\n",
    "            # Initial setup\n",
    "            seed = seed + repeat\n",
    "            \n",
    "            # Initialize variables\n",
    "            mu_0 = np.full(p, 70)\n",
    "            mu_hat, var_hat, obs_num = np.zeros(p), np.ones(p), np.zeros(p)\n",
    "            \n",
    "            # Initial observation\n",
    "            N_0 = math.ceil(p / m)\n",
    "            for i in range(N_0):\n",
    "                ids = efficient_initialization(p, m, obs_num, mu_0, std2, R2, Sigma2, J)\n",
    "                obs_num[ids] += 1\n",
    "                indices = ids.copy()\n",
    "                obs = np.random.multivariate_normal(mu[ids], Sigma2[np.ix_(ids, ids)])\n",
    "                X_n = ids.copy()\n",
    "                \n",
    "                # Update estimates\n",
    "                Gamma = Sigma2[np.ix_(ids, ids)]\n",
    "                inv_mat = np.linalg.inv(Sigma1[np.ix_(ids, ids)] + Gamma)\n",
    "                delta = (obs - mu_0[ids]).reshape(-1, 1)\n",
    "                \n",
    "                COV_x = Sigma1[:, ids]\n",
    "                adjustments = (COV_x @ inv_mat @ delta).flatten()\n",
    "                mu_hat = mu_0 + adjustments\n",
    "                adjustments = np.diag(COV_x @ inv_mat @ COV_x.T)\n",
    "                var_hat = np.diag(Sigma1) - adjustments\n",
    "                \n",
    "            # Main loop\n",
    "            G = [] #用于计算累积遗憾\n",
    "            G_pre = [] \n",
    "            for t in range(N_0+1, n_iter+1):\n",
    "                if beta == 'dynamic':\n",
    "                    dynamic_beta = np.sqrt(6 * np.log(t) / np.maximum(obs_num, 1))\n",
    "                    U, ids = improvement(p, m, mu_hat - dynamic_beta * np.sqrt(var_hat), std2, R2, Sigma2, J=J)\n",
    "                elif beta == 'cluster': \n",
    "                    ids = find_min_mu_in_clusters(mu_hat, cluster_labels)\n",
    "                    sub_Sigma_cluster = Sigma2[np.ix_(ids, ids)]\n",
    "                    U = np.linalg.cholesky(sub_Sigma_cluster) \n",
    "                else:\n",
    "                    ids = np.argsort(mu_hat)[:m]\n",
    "                    sub_Sigma_smallest = Sigma2[np.ix_(ids, ids)]\n",
    "                    U = np.linalg.cholesky(sub_Sigma_smallest) \n",
    "                obs_num[ids] += 1\n",
    "                indices = np.vstack((indices, ids))\n",
    "                \n",
    "                newobs = np.random.multivariate_normal(mu[ids], Sigma2[np.ix_(ids, ids)])\n",
    "                X_n, obs = update_subject_stats(obs_num, X_n, obs, ids, newobs)\n",
    "                \n",
    "                # Update Gamma matrix\n",
    "                len_Xn = len(X_n)\n",
    "                Gamma = np.zeros((len_Xn, len_Xn))\n",
    "                for i in range(len_Xn):\n",
    "                    for j in range(i, len_Xn):\n",
    "                        count = np.sum((indices == X_n[i]).any(1) & (indices == X_n[j]).any(1))\n",
    "                        Gamma[i,j] = Sigma2[X_n[i],X_n[j]] * count / (obs_num[X_n[i]] * obs_num[X_n[j]])\n",
    "                Gamma += Gamma.T\n",
    "                np.fill_diagonal(Gamma, np.diag(Gamma)/2)\n",
    "                \n",
    "                # Update estimates\n",
    "                COV_XX = Sigma1[np.ix_(X_n, X_n)]\n",
    "                inv_mat = np.linalg.inv(COV_XX + Gamma)\n",
    "                delta = (obs - mu_0[X_n]).reshape(-1, 1)\n",
    "                \n",
    "                COV_x = Sigma1[:, X_n]\n",
    "                adjustments = (COV_x @ inv_mat @ delta).flatten()\n",
    "                mu_hat = mu_0 + adjustments\n",
    "                adjustments = np.diag(COV_x @ inv_mat @ COV_x.T)\n",
    "                var_hat = np.diag(Sigma1) - adjustments\n",
    "\n",
    "                # Calculate G\n",
    "                z_samples = np.random.randn(J*10, m)\n",
    "                G.append(np.mean(np.min(mu[ids] + (U @ z_samples.T).T, axis=1)))\n",
    "\n",
    "                Gmin = 1000\n",
    "                idmin = []\n",
    "                z_samples2 = np.random.randn(J, m)\n",
    "                for id in indices:\n",
    "                    sub_Sigma = Sigma2[np.ix_(id, id)]\n",
    "                    B = np.linalg.cholesky(sub_Sigma)\n",
    "                    Gtemp = np.mean(np.min(mu_hat[id] + (B @ z_samples2.T).T, axis=1))\n",
    "                    if Gtemp < Gmin:\n",
    "                        idmin = id.copy()\n",
    "                        Gmin = Gtemp\n",
    "                ids1 = idmin\n",
    "\n",
    "                sub_Sigma = Sigma2[np.ix_(ids1, ids1)]\n",
    "                U1 = np.linalg.cholesky(sub_Sigma)\n",
    "                G_pre.append(np.mean(np.min(mu[ids1] + (U1 @ z_samples.T).T, axis=1)))\n",
    "            \n",
    "            # Save results\n",
    "            with open(csv_path, 'a') as f:\n",
    "                f.write(','.join([str(repeat)] + [f'{x:.6f}' for x in np.array(G)-G_real]) + '\\n')\n",
    "            with open(csv_path1, 'a') as f:\n",
    "                f.write(','.join([str(repeat)] + [f'{x:.6f}' for x in np.array(G_pre)-G_real]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd80efa",
   "metadata": {},
   "source": [
    "* worst: [31, 34, 29, 40, 24, 36, 43, 35]\n",
    "* random: [1, 20, 47, 35, 7, 8, 17, 37]\n",
    "* best: [10, 11, 48, 47, 9, 45, 43, 34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457bc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始课程组: [1, 20, 47, 35, 7, 8, 17, 37]\n",
      "\n",
      "=== 第 1 轮循环优化 ===\n",
      "\n",
      "优化第 8 门课程 (课程 37)\n",
      "找到 150 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 30, 改进分数: 3.536463\n",
      "更新后课程组: [30, 1, 20, 47, 35, 7, 8, 17]\n",
      "\n",
      "优化第 8 门课程 (课程 17)\n",
      "找到 651 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 17, 改进分数: 5.104656\n",
      "更新后课程组: [17, 30, 1, 20, 47, 35, 7, 8]\n",
      "\n",
      "优化第 8 门课程 (课程 8)\n",
      "找到 640 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 8, 改进分数: 3.817412\n",
      "更新后课程组: [8, 17, 30, 1, 20, 47, 35, 7]\n",
      "\n",
      "优化第 8 门课程 (课程 7)\n",
      "找到 788 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 7, 改进分数: 5.068350\n",
      "更新后课程组: [7, 8, 17, 30, 1, 20, 47, 35]\n",
      "\n",
      "优化第 8 门课程 (课程 35)\n",
      "找到 1113 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 35, 改进分数: 6.023646\n",
      "更新后课程组: [35, 7, 8, 17, 30, 1, 20, 47]\n",
      "\n",
      "优化第 8 门课程 (课程 47)\n",
      "找到 1276 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 47, 改进分数: 5.060848\n",
      "更新后课程组: [47, 35, 7, 8, 17, 30, 1, 20]\n",
      "\n",
      "优化第 8 门课程 (课程 20)\n",
      "找到 1777 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 20, 改进分数: 5.745176\n",
      "更新后课程组: [20, 47, 35, 7, 8, 17, 30, 1]\n",
      "\n",
      "优化第 8 门课程 (课程 1)\n",
      "找到 3099 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 1, 改进分数: 8.178115\n",
      "更新后课程组: [1, 20, 47, 35, 7, 8, 17, 30]\n",
      "\n",
      "=== 第 2 轮循环优化 ===\n",
      "\n",
      "优化第 8 门课程 (课程 30)\n",
      "找到 583 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 30, 改进分数: 3.841221\n",
      "更新后课程组: [30, 1, 20, 47, 35, 7, 8, 17]\n",
      "\n",
      "优化第 8 门课程 (课程 17)\n",
      "找到 641 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 17, 改进分数: 4.911143\n",
      "更新后课程组: [17, 30, 1, 20, 47, 35, 7, 8]\n",
      "\n",
      "优化第 8 门课程 (课程 8)\n",
      "找到 630 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 28, 改进分数: 4.103844\n",
      "更新后课程组: [28, 17, 30, 1, 20, 47, 35, 7]\n",
      "\n",
      "优化第 8 门课程 (课程 7)\n",
      "找到 843 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 7, 改进分数: 4.987148\n",
      "更新后课程组: [7, 28, 17, 30, 1, 20, 47, 35]\n",
      "\n",
      "优化第 8 门课程 (课程 35)\n",
      "找到 1195 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 35, 改进分数: 6.154927\n",
      "更新后课程组: [35, 7, 28, 17, 30, 1, 20, 47]\n",
      "\n",
      "优化第 8 门课程 (课程 47)\n",
      "找到 1272 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 47, 改进分数: 4.939164\n",
      "更新后课程组: [47, 35, 7, 28, 17, 30, 1, 20]\n",
      "\n",
      "优化第 8 门课程 (课程 20)\n",
      "找到 1802 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 20, 改进分数: 5.850654\n",
      "更新后课程组: [20, 47, 35, 7, 28, 17, 30, 1]\n",
      "\n",
      "优化第 8 门课程 (课程 1)\n",
      "找到 3025 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 1, 改进分数: 8.089193\n",
      "更新后课程组: [1, 20, 47, 35, 7, 28, 17, 30]\n",
      "\n",
      "=== 第 3 轮循环优化 ===\n",
      "\n",
      "优化第 8 门课程 (课程 30)\n",
      "找到 627 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 30, 改进分数: 3.846637\n",
      "更新后课程组: [30, 1, 20, 47, 35, 7, 28, 17]\n",
      "\n",
      "优化第 8 门课程 (课程 17)\n",
      "找到 671 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 17, 改进分数: 4.664614\n",
      "更新后课程组: [17, 30, 1, 20, 47, 35, 7, 28]\n",
      "\n",
      "优化第 8 门课程 (课程 28)\n",
      "找到 633 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 28, 改进分数: 4.205293\n",
      "更新后课程组: [28, 17, 30, 1, 20, 47, 35, 7]\n",
      "\n",
      "优化第 8 门课程 (课程 7)\n",
      "找到 761 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 7, 改进分数: 5.309904\n",
      "更新后课程组: [7, 28, 17, 30, 1, 20, 47, 35]\n",
      "\n",
      "优化第 8 门课程 (课程 35)\n",
      "找到 1183 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 35, 改进分数: 6.099361\n",
      "更新后课程组: [35, 7, 28, 17, 30, 1, 20, 47]\n",
      "\n",
      "优化第 8 门课程 (课程 47)\n",
      "找到 1278 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 47, 改进分数: 5.008858\n",
      "更新后课程组: [47, 35, 7, 28, 17, 30, 1, 20]\n",
      "\n",
      "优化第 8 门课程 (课程 20)\n",
      "找到 1841 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 20, 改进分数: 5.702128\n",
      "更新后课程组: [20, 47, 35, 7, 28, 17, 30, 1]\n",
      "\n",
      "优化第 8 门课程 (课程 1)\n",
      "找到 3052 个有效样本（最后一门课是最小值）\n",
      "最佳替换: 课程 1, 改进分数: 8.111263\n",
      "更新后课程组: [1, 20, 47, 35, 7, 28, 17, 30]\n",
      "无课程被替换，算法已收敛\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 20, 47, 35, 7, 28, 17, 30]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import multivariate_normal\n",
    "from util.data_generate import *\n",
    "from util.methods import *\n",
    "import csv\n",
    "#worst\n",
    "p = 50\n",
    "betas = ['smallest','cluster','dynamic']\n",
    "seed, J, n_iter, num_repeats = 1, 10000, 150, 30\n",
    "np.random.seed(seed=seed)\n",
    "Sigma1 = np.loadtxt('Data_generate/Sigma1.txt')\n",
    "Sigma1 = Sigma1 * 4\n",
    "\n",
    "Sigma2 = np.loadtxt('Data_generate/group_cov.txt')\n",
    "std2 = np.sqrt(np.diag(Sigma2))\n",
    "R2 = Sigma2 / np.outer(std2, std2)\n",
    "mu = np.loadtxt('Data_generate/mu.txt')\n",
    "\n",
    "ids1 = [1,2,3,4,5,6,7,8]\n",
    "A, B = cyclic_optimization_persistent(mu, Sigma2, ids1, list(range(p)))\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc0b4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(52.875085591386565)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_samples2 = np.random.randn(100000, 8)\n",
    "ids1 = [1, 20, 47, 35, 7, 8, 17, 37]\n",
    "sub_Sigma = Sigma2[np.ix_(ids1, ids1)]\n",
    "U1 = np.linalg.cholesky(sub_Sigma)\n",
    "e1 = (np.mean(np.min(mu[ids1] + (U1 @ z_samples2.T).T, axis=1)))\n",
    "e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc38ff4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(52.67687475785763)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids2 = [1, 20, 47, 35, 7, 28, 17, 30]\n",
    "sub_Sigma = Sigma2[np.ix_(ids2, ids2)]\n",
    "U2 = np.linalg.cholesky(sub_Sigma)\n",
    "e2 = (np.mean(np.min(mu[ids2] + (U2 @ z_samples2.T).T, axis=1)))\n",
    "e2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
